---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Ben Zhang, bzhang369
    - Partner 2 (name and cnet ID): Xinyi Chen, chen61
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: XC, BZ
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  : XC, BZ(1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

enforcement_actions = soup.find_all('div', class_='usa-card__container')

titles = []
dates = []
categories = []
links = []

for action in enforcement_actions:
    
    title_tag = action.find('h2', class_='usa-card__heading').find('a')
    title = title_tag.text.strip()
    link = "https://oig.hhs.gov" + title_tag['href']
    
    date = action.find('span', class_='text-base-dark padding-right-105').text.strip()
    
    category_tag = action.find('ul', class_='display-inline add-list-reset')
    category = category_tag.find('li').text.strip()
    
    titles.append(title)
    dates.append(date)
    categories.append(category)
    links.append(link)

df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

print(df.head())

```

### 2. Crawling (PARTNER 1)

```{python}
agencies = []
for link in links:
    response = requests.get(link)
    soup = BeautifulSoup(response.content, 'html.parser')

    agency_tag = soup.find('span', class_='padding-right-2 text-base')
    if agency_tag:
        agency_text = agency_tag.find_next('li').text.strip()
        if agency_text.startswith("Agency:"):
            agency = agency_text.replace("Agency:", "").strip()
        else:
            agency = "Unknown"  # If "Agency" prefix is not present, assign "Unknown"
    else:
        agency = "Unknown"  

    agencies.append(agency)

df['Agency'] = agencies

print(df.head())

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

1. Define the main function scrape_enforcement_actions(start_year, start_month)

Check if start_year is valid (>= 2013). If not, return a message indicating the restriction.

2. Initialize variables

Create empty lists for titles, dates, categories, links, and agencies.

Set start_date based on start_year and start_month.

Set page to 1 and more_pages to True.

Initialize agency_cache as an empty dictionary.

3. First Loop to Iterate Pages

While there are more pages and the current page is <= 480:
Build the URL for the current page.
Send a request to the URL and parse the content using BeautifulSoup.
Extract enforcement actions using HTML tags.

4. Check if there are enforcement actions

If no enforcement actions are found, set more_pages to False and exit the loop.

5. Second Loop through enforcement actions 

For each enforcement action: Extract the date and check if it’s older than start_date. If so, stop further pages.

Extract details (title, link, category).

Check if the agency information for the link is cached. If not:
Send a request to the link's page and scrape the agency information.

Cache the agency information to avoid duplicate requests.

Append the extracted data to the lists.

6. Move to the next page by incrementing page.

7. Save data to CSV

Create a DataFrame and save it to a CSV file.
Print the total number of enforcement actions scraped.

8. Return the DataFrame


* b. Create Dynamic Scraper (PARTNER 2)


```{python}
from datetime import datetime

def scrape_enforcement_actions(start_year, start_month):
    if start_year < 2013:
        print("Data is only available for years >= 2013.")
        return None

    titles, dates, categories, links, agencies = [], [], [], [], []
    start_date = datetime(start_year, start_month, 1)
    page = 1
    more_pages = True
    agency_cache = {}

    while more_pages and page <= 480:
        url = f"https://oig.hhs.gov/fraud/enforcement/?page={page}"
        print(f"Scraping page {page}...")

        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        enforcement_actions = soup.find_all('div', class_='usa-card__container')

        if not enforcement_actions:
            more_pages = False
            break

        for action in enforcement_actions:
            date_text = action.find('span', class_='text-base-dark padding-right-105').text.strip()
            action_date = datetime.strptime(date_text, "%B %d, %Y")
            
            if action_date < start_date:
                more_pages = False
                break

            title_tag = action.find('h2', class_='usa-card__heading').find('a')
            title = title_tag.text.strip()
            link = "https://oig.hhs.gov" + title_tag['href']
            category_tag = action.find('ul', class_='display-inline add-list-reset')
            category = category_tag.find('li').text.strip()

            titles.append(title)
            dates.append(date_text)
            categories.append(category)
            links.append(link)

# Agency Caching: if an agency’s page has already been accessed, the cached value will be used, reducing the number of detail page requests.
            if link in agency_cache:
                agency = agency_cache[link]
            else:
                agency_response = requests.get(link)
                agency_soup = BeautifulSoup(agency_response.content, 'html.parser')
                agency_tag = agency_soup.find('span', class_='padding-right-2 text-base')
                if agency_tag:
                    agency_text = agency_tag.find_next('li').text.strip()
                    if agency_text.startswith("Agency:"):
                        agency = agency_text.replace("Agency:", "").strip()
                    else:
                        agency = "Unknown"
                else:
                    agency = "Unknown"
                agency_cache[link] = agency  # Cache the result to avoid future requests
            agencies.append(agency)

        page += 1

    df = pd.DataFrame({
        'Title': titles,
        'Date': dates,
        'Category': categories,
        'Link': links,
        'Agency': agencies
    })
    df.to_csv(f"enforcement_actions_{start_year}_{start_month}.csv", index=False)
    print(f"Data saved to enforcement_actions_{start_year}_{start_month}.csv")

    return df

# ChatGpt: Can you help me reduces the time of running this loop?, agency_cache to store agency information by link.
```

```{python}
df_2023 = scrape_enforcement_actions(2023, 1)
print(f"Total enforcement actions: {len(df_2023)}")
print(f"The last row is : {df_2023.tail(1)}")
```

* c. Test Partner's Code (PARTNER 1)

```{python}
df_2021 = scrape_enforcement_actions(2021, 1)
print(f"Total enforcement actions: {len(df_2021)}")
print(f"The last row is : {df_2021.tail(1)}")
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
df_2021 = pd.read_csv("enforcement_actions_2021_1.csv")

df_2021['Date'] = pd.to_datetime(df_2021['Date'], format="%B %d, %Y")

df_2021['Year-Month'] = df_2021['Date'].dt.to_period('M')

monthly_counts = df_2021.groupby('Year-Month').size().reset_index(name='Count')
monthly_counts['Year-Month'] = monthly_counts['Year-Month'].dt.to_timestamp()  # 

alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('Year-Month:T', title='Date', axis=alt.Axis(format='%Y-%m')),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions')
).properties(
    title='Monthly Enforcement Actions Since January 2021',
    width=600,
    height=400
)
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
df_2021['Year-Month'] = df_2021['Date'].dt.to_period('M').dt.to_timestamp()

df_filtered = df_2021[df_2021['Category'].isin(["Criminal and Civil Actions", "State Enforcement Agencies"])]

category_counts = df_filtered.groupby(['Year-Month', 'Category']).size().reset_index(name='Count')

alt.Chart(category_counts).mark_line().encode(
    x=alt.X('Year-Month:T', title='Date', axis=alt.Axis(format='%Y-%m')),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions'),
    color=alt.Color('Category:N', title='Category'),
    tooltip=['Year-Month:T', 'Count:Q', 'Category:N']
).properties(
    title='Monthly Enforcement Actions: Criminal and Civil Actions vs. State Enforcement Agencies',
    width=600,
    height=400
)
```

* based on five topics

```{python}
df_criminal_civil = df_filtered[df_filtered['Category'] == "Criminal and Civil Actions"]

def assign_topic(title):
    title_lower = title.lower()
    if "health" in title_lower or "medi" in title_lower:
        return "Health Care Fraud"
    elif "bank" in title_lower or "financial" in title_lower or "laundering" in title_lower or "tax" in title_lower :
        return "Financial Fraud"
    elif "drug" in title_lower or "opioid" in title_lower or "substance" in title_lower:
        return "Drug Enforcement"
    elif "bribe" in title_lower or "corruption" in title_lower or "kickback" in title_lower:
        return "Bribery/Corruption"
    else:
        return "Other"

df_criminal_civil['Topic'] = df_criminal_civil['Title'].apply(assign_topic)

topic_counts = df_criminal_civil.groupby(['Year-Month', 'Topic']).size().reset_index(name='Count')

alt.Chart(topic_counts).mark_line().encode(
    x=alt.X('Year-Month:T', title='Date', axis=alt.Axis(format='%Y-%m')),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions'),
    color=alt.Color('Topic:N', title='Topic')
).properties(
    title='Monthly Enforcement Actions by Topic in Criminal and Civil Actions',
    width=600,
    height=400
)
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

state_gdf = gpd.read_file("cb_2018_us_state_500k.shp")
enforcement_data = pd.read_csv("enforcement_actions_2021_1.csv")  

state_enforcement = enforcement_data[enforcement_data['Agency'].str.contains("State of", na=False)]
state_enforcement['State'] = state_enforcement['Agency'].str.replace("State of", "").str.strip()

state_counts = state_enforcement['State'].value_counts().reset_index()
state_counts.columns = ['State', 'EnforcementCount']

state_gdf = state_gdf.rename(columns={"NAME": "State"})  
merged_gdf = state_gdf.merge(state_counts, on="State", how="left").fillna(0) 

fig, ax = plt.subplots(1, 1, figsize=(15, 10))
merged_gdf.plot(column="EnforcementCount", cmap="Blues", linewidth=0.8, ax=ax, edgecolor="0.8", legend=True)
ax.set_title("Enforcement Actions by State", fontsize=16)
ax.set_axis_off()

# ChatGpt, "how can I make my graph bigger and remove Alska state"

plt.axis('equal')  # Keeps the aspect ratio correct
plt.xlim([-130, -60])  # Focus on the contiguous U.S.
plt.ylim([20, 55])

plt.show()

```


### 2. Map by District (PARTNER 2)

```{python}
district_gdf = gpd.read_file("geo_export_828db5ef-6c4f-412d-aed1-b5f6c9905efd.shp")

enforcement_data = pd.read_csv("enforcement_actions_2021_1.csv")

district_enforcement = enforcement_data[enforcement_data['Agency'].str.contains("District", na=False)]

# Extract the district name from the 'Agency' column using regex
district_enforcement['District'] = district_enforcement['Agency'].str.split(',').str[1].str.strip()

# Clean the district names further if necessary, e.g., remove extra text or symbols
district_enforcement['District'] = district_enforcement['District'].str.replace(r'[^\w\s,.-]', '', regex=True)

# Assuming the district shapefile has a 'Judicial District' column to match
district_gdf = district_gdf.rename(columns={"judicial_d": "District"})  # Adjust the column name to match

# Count the number of enforcement actions by district
district_counts = district_enforcement['District'].value_counts().reset_index()
district_counts.columns = ['District', 'EnforcementCount']

# Merge the cleaned district data with the district shapefile
merged_district_gdf = district_gdf.merge(district_counts, on="District", how="left").fillna(0)

# Plot the choropleth map for enforcement actions by district
fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # Adjust the figure size
merged_district_gdf.plot(
    column="EnforcementCount", 
    cmap="Blues", 
    linewidth=0.8, 
    ax=ax, 
    edgecolor="0.8", 
    legend=True
)

ax.set_title("Enforcement Actions by US Attorney District", fontsize=16)
ax.set_axis_off()  # Turn off the axis for better presentation

plt.axis('equal')  # Keeps the aspect ratio correct
plt.xlim([-130, -60])  # Focus on the contiguous U.S.
plt.ylim([20, 55])

plt.show()
```


## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
zip_codes = gpd.read_file("gz_2010_us_860_00_500k.shp")
population_2020 = pd.read_csv("DECENNIALDHC2020.P1-Data.csv")

population_2020  = population_2020.drop(index=0).reset_index(drop=True)

population_2020.rename(columns={'NAME': 'ZCTA5'}, inplace=True)

population_2020 ['ZCTA5'] = population_2020 ['ZCTA5'].str.replace('ZCTA5 ', '', regex=False)

population_2020 = population_2020.drop(columns=['Unnamed: 3'])

merged_2020 = population_2020.merge(zip_codes, left_on='ZCTA5', right_on='ZCTA5', how='left')

print(merged_2020.head(3))
```

### 2. Conduct spatial join
```{python}
merged_2020['P1_001N'] = pd.to_numeric(merged_2020['P1_001N'])

merged_2020 = gpd.GeoDataFrame(merged_2020, geometry='geometry')

merged_2020 = merged_2020.set_crs(zip_codes.crs)
merged_2020 = merged_2020.to_crs(district_gdf.crs)

joined = gpd.sjoin(merged_2020, district_gdf, how="left", predicate="intersects")

district_population = joined.groupby('District')['P1_001N'].sum().reset_index()
district_population.rename(columns={'P1_001N': 'Total_Population'}, inplace=True)

print(district_population)

```

### 3. Map the action ratio in each district
```{python}

```