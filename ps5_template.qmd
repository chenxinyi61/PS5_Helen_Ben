---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

enforcement_actions = soup.find_all('div', class_='usa-card__container')

titles = []
dates = []
categories = []
links = []

for action in enforcement_actions:
    
    title_tag = action.find('h2', class_='usa-card__heading').find('a')
    title = title_tag.text.strip()
    link = "https://oig.hhs.gov" + title_tag['href']
    
    date = action.find('span', class_='text-base-dark padding-right-105').text.strip()
    
    category_tag = action.find('ul', class_='display-inline add-list-reset')
    category = category_tag.find('li').text.strip()
    
    titles.append(title)
    dates.append(date)
    categories.append(category)
    links.append(link)

df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

print(df.head())

```

### 2. Crawling (PARTNER 1)

```{python}
agencies = []
for link in links:
    response = requests.get(link)
    soup = BeautifulSoup(response.content, 'lxml')

    agency_tag = soup.find('span', class_='padding-right-2 text-base')

    if agency_tag:
        agency_text = agency_tag.find_next('li').text.strip()
        if agency_text.startswith("Agency:"):

            agency = agency_text.replace("Agency:", "").strip()
        else:

            agency = "Unknown"
    else:

        agency = "Unknown"

    agencies.append(agency)

df['Agency'] = agencies

print(df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime

def scrape_enforcement_actions(start_year, start_month):
    if start_year < 2013:
        print("Data is only available for years >= 2013.")
        return None

    titles, dates, categories, links, agencies = [], [], [], [], []
    start_date = datetime(start_year, start_month, 1)
    page = 1
    more_pages = True
    agency_cache = {}

    while more_pages and page <= 480:
        url = f"https://oig.hhs.gov/fraud/enforcement/?page={page}"
        print(f"Scraping page {page}...")

        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        enforcement_actions = soup.find_all('div', class_='usa-card__container')

        if not enforcement_actions:
            more_pages = False
            break

        for action in enforcement_actions:
            date_text = action.find('span', class_='text-base-dark padding-right-105').text.strip()
            action_date = datetime.strptime(date_text, "%B %d, %Y")
            
            if action_date < start_date:
                more_pages = False
                break

            title_tag = action.find('h2', class_='usa-card__heading').find('a')
            title = title_tag.text.strip()
            link = "https://oig.hhs.gov" + title_tag['href']
            category_tag = action.find('ul', class_='display-inline add-list-reset')
            category = category_tag.find('li').text.strip()

            titles.append(title)
            dates.append(date_text)
            categories.append(category)
            links.append(link)

            if link in agency_cache:
                agency = agency_cache[link]
            else:
                # Fetch agency from detail page only when necessary
                agency_response = requests.get(link)
                agency_soup = BeautifulSoup(agency_response.content, 'html.parser')
                agency_tag = agency_soup.find('span', class_='padding-right-2 text-base')
                if agency_tag:
                    agency_text = agency_tag.find_next('li').text.strip()
                    if agency_text.startswith("Agency:"):
                        agency = agency_text.replace("Agency:", "").strip()
                    else:
                        agency = "Unknown"
                else:
                    agency = "Unknown"
                agency_cache[link] = agency  # Cache the result to avoid future requests

            agencies.append(agency)

        page += 1

    df = pd.DataFrame({
        'Title': titles,
        'Date': dates,
        'Category': categories,
        'Link': links,
        'Agency': agencies
    })
    df.to_csv(f"enforcement_actions_{start_year}_{start_month}.csv", index=False)
    print(f"Data saved to enforcement_actions_{start_year}_{start_month}.csv")
    
    print(f"Total enforcement actions: {len(df)}")
    if not df.empty:
        earliest_action = df.sort_values(by='Date').iloc[0]
        print("Earliest enforcement action scraped:")
        print(earliest_action)

    return df

df = scrape_enforcement_actions(2023, 1)
```

```{python}
print(f"Total enforcement actions: {len(df)}")
print(f"The last row is : {df.tail(1)}")
```
* c. Test Partner's Code (PARTNER 1)

```{python}
df_2021 = scrape_enforcement_actions(2021, 1)
print(f"Total enforcement actions: {len(df_2021)}")
print(f"The last row is : {df_2021.tail(1)}")
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
df_2021 = pd.read_csv("enforcement_actions_2021_1.csv")

df_2021['Date'] = pd.to_datetime(df_2021['Date'], format="%B %d, %Y")

df_2021['Year-Month'] = df_2021['Date'].dt.to_period('M')

monthly_counts = df_2021.groupby('Year-Month').size().reset_index(name='Count')
monthly_counts['Year-Month'] = monthly_counts['Year-Month'].dt.to_timestamp()  

alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('Year-Month:T', title='Date', axis=alt.Axis(format='%Y-%m')),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions')
).properties(
    title='Monthly Enforcement Actions Since January 2021',
    width=600,
    height=400
)
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
import altair as alt

filtered_df_1 = df_2021[df_2021['Category'].isin(["Criminal and Civil Actions", "State Enforcement Agencies"])]
category_counts = (
    filtered_df_1
    .groupby([filtered_df_1["Date"].dt.to_period("M"), "Category"])
    .size()
    .reset_index(name="Count")
)
category_counts["Date"] = category_counts["Date"].dt.to_timestamp()

chart_1 = alt.Chart(category_counts).mark_line(point=True).encode(
    x=alt.X("Date:T", title="Date"),
    y=alt.Y("Count:Q", title="Number of Enforcement Actions"),
    color=alt.Color("Category:N", title="Category"),
    tooltip=["Date:T", "Category:N", "Count:Q"]
).properties(
    title="Comparison of Enforcement Actions: Criminal and Civil Actions vs. State Enforcement Agencies",
    width=600,
    height=400
).interactive()

chart_1

```

* based on five topics

```{python}
def assign_topic(row):
    title = row["Title"].lower()
    if "health" in title or "medi" in title:
        return "Health Care Fraud"
    elif "bank" in title or "financial" in title or "laundering" in title or "tax" in title:
        return "Financial Fraud"
    elif "drug" in title or "narcotics" in title or "substance" in title or "opioid" in title:
        return "Drug Enforcement"
    elif "bribe" in title or "corruption" in title or "kickback" in title:
        return "Bribery/Corruption"
    else:
        return "Other"

filtered_df_2 = filtered_df_1[filtered_df_1["Category"] == "Criminal and Civil Actions"]
filtered_df_2["Topic"] = filtered_df_2.apply(assign_topic, axis=1)

topic_counts = (
    filtered_df_2
    .groupby([filtered_df_2["Date"].dt.to_period("M"), "Topic"])
    .size()
    .reset_index(name="Count")
)
topic_counts["Date"] = topic_counts["Date"].dt.to_timestamp()

chart_2 = alt.Chart(topic_counts).mark_line(point=True).encode(
    x=alt.X("Date:T", title="Date"),
    y=alt.Y("Count:Q", title="Number of Enforcement Actions"),
    color=alt.Color("Topic:N", title="Topic"),
    tooltip=["Date:T", "Topic:N", "Count:Q"]
).properties(
    title="Breakdown of Enforcement Actions in Criminal and Civil Actions by Topic",
    width=600,
    height=400
)

chart_2

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

state_gdf = gpd.read_file("cb_2018_us_state_500k.shp")
enforcement_data = pd.read_csv("enforcement_actions_2021_1.csv")  

state_enforcement = enforcement_data[enforcement_data['Agency'].str.contains("State of", na=False)]
state_enforcement['State'] = state_enforcement['Agency'].str.replace("State of", "").str.strip()

state_counts = state_enforcement['State'].value_counts().reset_index()
state_counts.columns = ['State', 'EnforcementCount']

state_gdf = state_gdf.rename(columns={"NAME": "State"})  
merged_gdf = state_gdf.merge(state_counts, on="State", how="left").fillna(0) 

fig, ax = plt.subplots(1, 1, figsize=(15, 10))
merged_gdf.plot(column="EnforcementCount", cmap="Blues", linewidth=0.8, ax=ax, edgecolor="0.8", legend=True)
ax.set_title("Enforcement Actions by State", fontsize=16)
ax.set_axis_off()

plt.axis('equal')  
plt.xlim([-180, -60])  

plt.show()


```

### 2. Map by District (PARTNER 2)

```{python}
district_gdf = gpd.read_file("geo_export_f076858e-676a-406c-8696-324e40336d68.shp")

enforcement_data = pd.read_csv("enforcement_actions_2021_1.csv")

district_enforcement = enforcement_data[enforcement_data['Agency'].str.contains("District", na=False)]

district_enforcement['District'] = district_enforcement['Agency'].str.split(',').str[1].str.strip()

district_enforcement['District'] = district_enforcement['District'].str.replace(r'[^\w\s,.-]', '', regex=True)
 
district_gdf = district_gdf.rename(columns={"judicial_d": "District"}) 


district_counts = district_enforcement['District'].value_counts().reset_index()
district_counts.columns = ['District', 'EnforcementCount']

merged_district_gdf = district_gdf.merge(district_counts, on="District", how="left").fillna(0)

fig, ax = plt.subplots(1, 1, figsize=(12, 8))  
merged_district_gdf.plot(
    column="EnforcementCount", 
    cmap="Blues", 
    linewidth=0.8, 
    ax=ax, 
    edgecolor="0.8", 
    legend=True
)

ax.set_title("Enforcement Actions by US Attorney District", fontsize=16)
ax.set_axis_off()  

plt.axis('equal')  # Keeps the aspect ratio correct
plt.xlim([-180, -60])  
plt.ylim([20, 55])

plt.show()
```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}
zip_codes = gpd.read_file("gz_2010_us_860_00_500k.shp")

population_2020 = pd.read_csv("DECENNIALDHC2020.P1-Data.csv")

population_2020  = population_2020 .drop(index=0).reset_index(drop=True)

population_2020.rename(columns={'NAME': 'ZCTA5'}, inplace=True)

population_2020 ['ZCTA5'] = population_2020 ['ZCTA5'].str.replace('ZCTA5 ', '', regex=False)

population_2020 = population_2020.drop(columns=['Unnamed: 3'])

merged_2020 = population_2020.merge(zip_codes, left_on='ZCTA5', right_on='ZCTA5', how='left')

print(merged_2020.head(3))
```

### 2. Conduct spatial join
```{python}
merged_2020['P1_001N'] = pd.to_numeric(merged_2020['P1_001N'])

merged_2020 = gpd.GeoDataFrame(merged_2020, geometry='geometry', crs=zip_codes.crs)

if merged_2020.crs != district_gdf.crs:
    merged_2020 = merged_2020.to_crs(district_gdf.crs)

joined = gpd.sjoin(merged_2020, district_gdf, how="left", predicate="intersects")

district_population = joined.groupby('District')['P1_001N'].sum().reset_index()
district_population.rename(columns={'P1_001N': 'Total_Population'}, inplace=True)

print(district_population)
```

### 3. Map the action ratio in each district
```{python}
merged_district_gdf['Total_Population'] = pd.to_numeric(merged_district_gdf['Total_Population'], errors='coerce')

merged_district_gdf['EnforcementCount'] = merged_district_gdf['EnforcementCount'].fillna(0)
merged_district_gdf['Total_Population'] = merged_district_gdf['Total_Population'].fillna(1)  # Avoid division by zero

merged_district_gdf['EnforcementPerCapita'] = merged_district_gdf['EnforcementCount'] / merged_district_gdf['Total_Population']

print(merged_district_gdf[['District', 'EnforcementCount', 'Total_Population', 'EnforcementPerCapita']].head())

fig, ax = plt.subplots(1, 1, figsize=(12, 8)) 
merged_district_gdf.plot(
    column='EnforcementPerCapita',
    cmap='Blues',
    linewidth=0.8,
    ax=ax,
    edgecolor='0.8',
    legend=True
)

ax.set_title("Enforcement Actions per Capita by US Attorney District", fontsize=16)
ax.set_axis_off() 

plt.axis('equal')  # Keeps the aspect ratio correct
plt.xlim([-180, -60]) 
plt.ylim([20, 55])

plt.show()
```